{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import heapq\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from qnetwork import QNetwork\n",
    "from experience import Experience, get_fields_from_experiences\n",
    "from utils import plot_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Seed: 5\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "\n",
    "seed = 5\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "print('Seed:', seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brain name: BananaBrain\n",
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "State example: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "State size: 37\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Banana_Linux/Banana.x86_64')\n",
    "\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "print('Brain name:', brain_name)\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "state = env_info.vector_observations[0]\n",
    "print('State example:', state)\n",
    "\n",
    "state_size = len(state)\n",
    "print('State size:', state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>priority exponent</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.25</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.75</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>segment boundaries</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.757</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "priority exponent   0.00   0.25   0.50   0.75  1.00\n",
       "segment boundaries                                 \n",
       "0                    0.0  0.000  0.000  0.000  0.00\n",
       "1                    0.2  0.134  0.089  0.060  0.04\n",
       "2                    0.4  0.318  0.253  0.201  0.16\n",
       "3                    0.6  0.528  0.465  0.409  0.36\n",
       "4                    0.8  0.757  0.716  0.677  0.64\n",
       "5                    1.0  1.000  1.000  1.000  1.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_priority_segments(num_segments, priority_exp):\n",
    "    return [(i / num_segments) ** (priority_exp + 1) for i in range(num_segments + 1)]\n",
    " \n",
    "def demo_compute_priority_segments():\n",
    "    priority_exps = [0, 0.25, 0.5, 0.75, 1]\n",
    "    segments = {e: compute_priority_segments(5, e) for e in priority_exps}\n",
    "    \n",
    "    df = pd.DataFrame(segments)\n",
    "    df.columns.name = 'priority exponent'\n",
    "    df.index.name = 'segment boundaries'\n",
    "    pd.set_option('precision', 3)\n",
    "    display(df)\n",
    "        \n",
    "demo_compute_priority_segments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>priority exponent</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.25</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.75</th>\n",
       "      <th>0.99</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "priority exponent  0.00   0.25   0.50   0.75   0.99\n",
       "rank                                               \n",
       "1                  0.01  0.024  0.056  0.116  0.212\n",
       "20                 0.01  0.012  0.012  0.012  0.011\n",
       "40                 0.01  0.010  0.009  0.007  0.006\n",
       "60                 0.01  0.009  0.007  0.005  0.004\n",
       "80                 0.01  0.008  0.006  0.004  0.003\n",
       "100                0.01  0.008  0.006  0.004  0.002"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_rank_probs(ranks, memory_size, priority_exp):\n",
    "    return (1 - priority_exp) / ranks**priority_exp / (memory_size**(1 - priority_exp) - 1)\n",
    "\n",
    "def demo_compute_rank_probs():\n",
    "    ranks = np.array([1, 20, 40, 60, 80, 100])\n",
    "    priority_exps = [0, 0.25, 0.5, 0.75, 0.99]\n",
    "    rank_probs = {e: compute_rank_probs(ranks, 100, e) for e in priority_exps}\n",
    "    \n",
    "    df = pd.DataFrame(rank_probs, index=ranks)\n",
    "    df.columns.name = 'priority exponent'\n",
    "    df.index.name = 'rank'\n",
    "    pd.set_option('precision', 3)\n",
    "    display(df)\n",
    "\n",
    "demo_compute_rank_probs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>sample exponent</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.25</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.75</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "sample exponent  0.00   0.25   0.50   0.75   1.00\n",
       "rank                                             \n",
       "1                 1.0  0.562  0.316  0.178  0.100\n",
       "20                1.0  0.818  0.669  0.547  0.447\n",
       "40                1.0  0.892  0.795  0.709  0.632\n",
       "60                1.0  0.938  0.880  0.826  0.775\n",
       "80                1.0  0.972  0.946  0.920  0.894\n",
       "100               1.0  1.000  1.000  1.000  1.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_sample_weights(sample_probs, memory_size, priority_exp, sample_exp):\n",
    "    min_sample_prob = compute_rank_probs(memory_size, memory_size, priority_exp)\n",
    "    max_sample_weight = (memory_size * min_sample_prob)**(-sample_exp)\n",
    "    sample_weights = (memory_size * sample_probs)**(-sample_exp) / max_sample_weight\n",
    "    return sample_weights\n",
    "    \n",
    "def demo_compute_sample_weights():\n",
    "    memory_size = 100\n",
    "    priority_exp = 0.5\n",
    "    ranks = np.array([1, 20, 40, 60, 80, 100])\n",
    "    sample_exps = [0, 0.25, 0.5, 0.75, 1]\n",
    "    sample_probs = compute_rank_probs(ranks, memory_size, priority_exp)\n",
    "    sample_weights = {e: compute_sample_weights(sample_probs, memory_size, priority_exp, e) for e in sample_exps}\n",
    "\n",
    "    df = pd.DataFrame(sample_weights, index=ranks)\n",
    "    df.index.name = 'rank'\n",
    "    df.columns.name = 'sample exponent'\n",
    "    pd.set_option('precision', 3)\n",
    "    display(df)\n",
    "\n",
    "demo_compute_sample_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>997</td>\n",
       "      <td>0.416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>988</td>\n",
       "      <td>0.514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>978</td>\n",
       "      <td>0.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>908</td>\n",
       "      <td>0.707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>866</td>\n",
       "      <td>0.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>696</td>\n",
       "      <td>0.828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>686</td>\n",
       "      <td>0.832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>39</td>\n",
       "      <td>0.884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0.924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   error  weight\n",
       "0    997   0.416\n",
       "1    988   0.514\n",
       "2    978   0.562\n",
       "3    908   0.707\n",
       "4    866   0.740\n",
       "5    696   0.828\n",
       "6    686   0.832\n",
       "7     39   0.884\n",
       "8     27   0.924\n",
       "9      9   0.977"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PriorityReplayMemory:\n",
    "    def __init__(self, memory_size, batch_size, priority_exp, sort_every):\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.priority_exp = priority_exp\n",
    "        self.sort_every = sort_every\n",
    "        \n",
    "        self.memory = []\n",
    "        self.step = 0\n",
    "        \n",
    "        self.segments = compute_priority_segments(batch_size, priority_exp)\n",
    "        self.counter = itertools.count() \n",
    "    \n",
    "    def add(self, experience, error=None):\n",
    "        if error is None:\n",
    "            if len(self.memory) > 0:\n",
    "                priority = self.memory[0][0]\n",
    "            else:\n",
    "                priority = -10\n",
    "        else:\n",
    "            priority = -error\n",
    "\n",
    "        count = next(self.counter)\n",
    "        heapq.heappush(self.memory, (priority, count, experience))\n",
    "        \n",
    "        if len(self.memory) > self.memory_size:\n",
    "            self.memory.pop()\n",
    "            \n",
    "        self.step = (self.step + 1) % self.sort_every\n",
    "        \n",
    "        if self.step == 0:\n",
    "            self.memory.sort(key=lambda x: x[0])\n",
    "            \n",
    "    \n",
    "    def sample(self, sample_exp):\n",
    "        size = len(self.memory)\n",
    "\n",
    "        # Scale segements to the number of experiences\n",
    "        segments = [(math.ceil(start * (size - 1)), math.floor(end * (size - 1)))\n",
    "                    for start, end in zip(self.segments[:-1], self.segments[1:])]\n",
    "        \n",
    "        # Check if the first (smallest) segment contains more than one experience\n",
    "        if segments[0][0] < segments[0][1]:\n",
    "            indexes = [random.randint(start, end) for start, end in segments]\n",
    "        else: # Ignore segments, do uniform sampling\n",
    "            indexes = random.sample(range(size), self.batch_size)\n",
    "            \n",
    "        # Select experiences\n",
    "        experiences = [self.memory[i][2] for i in indexes]\n",
    "        \n",
    "        # Compute importance-sampling weights\n",
    "        ranks = np.array(indexes) + 1\n",
    "        probs = compute_rank_probs(ranks, self.memory_size, self.priority_exp)\n",
    "        weights = compute_sample_weights(probs, self.memory_size, self.priority_exp, sample_exp)\n",
    "            \n",
    "        # Remove sampled experiences\n",
    "        for index in sorted(indexes, reverse=True):\n",
    "            del self.memory[index]\n",
    "            \n",
    "        heapq.heapify(self.memory)\n",
    "        \n",
    "        return experiences, weights\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "def demo_priority_replay_memory():\n",
    "    memory = PriorityReplayMemory(memory_size=100, batch_size=10, priority_exp=0.5, sort_every=10)\n",
    "    \n",
    "    for i in range(1000):\n",
    "        memory.add(i, i)\n",
    "\n",
    "    exps, probs = memory.sample(0.5)\n",
    "    df = pd.DataFrame({'error': exps, 'weight': probs})\n",
    "    pd.set_option('precision', 3)\n",
    "    return df\n",
    "    \n",
    "demo_priority_replay_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriorityReplayAgent:\n",
    "    def __init__(self, \n",
    "                 state_size, \n",
    "                 action_size,\n",
    "                 memory_size,\n",
    "                 batch_size, \n",
    "                 priority_exp,\n",
    "                 sort_every,\n",
    "                 learn_every,\n",
    "                 learn_rate,\n",
    "                 discount_factor,\n",
    "                 soft_update_factor\n",
    "                ):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.priority_exp = priority_exp\n",
    "        self.learn_every = learn_every\n",
    "        self.discount_factor = discount_factor\n",
    "        self.soft_update_factor = soft_update_factor\n",
    "\n",
    "        self.local_model = QNetwork(state_size, action_size).to(device)\n",
    "        self.target_model = QNetwork(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.local_model.parameters(), lr=learn_rate)\n",
    "        self.memory = PriorityReplayMemory(memory_size, batch_size, priority_exp, sort_every)\n",
    "        \n",
    "        self.step = 0\n",
    "    \n",
    "    def perceive(self, state, action, reward, next_state, done, sample_exp):\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        self.memory.add(experience)\n",
    "        \n",
    "        self.step = (self.step + 1) % self.learn_every\n",
    "        \n",
    "        if self.step == 0 and len(self.memory) >= self.batch_size:\n",
    "            self.learn(sample_exp)\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        self.local_model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_values = self.local_model(state)\n",
    "        \n",
    "        self.local_model.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > epsilon:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, sample_exp):\n",
    "        experiences, sample_weights = self.memory.sample(sample_exp)\n",
    "        states, actions, rewards, next_states, dones = get_fields_from_experiences(experiences, device)\n",
    "        \n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.target_model(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards + self.discount_factor * Q_targets_next * (1 - dones)\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.local_model(states).gather(1, actions)\n",
    "        # Compute TD errors\n",
    "        td_errors = Q_targets - Q_expected\n",
    "        \n",
    "        # Re-add sampled experiences with new priorities\n",
    "        priorities = td_errors.detach().cpu().abs().squeeze().numpy()\n",
    "        \n",
    "        for experience, priority in zip(experiences, priorities):\n",
    "            self.memory.add(experience, priority)\n",
    "        \n",
    "        # Compute loss with importance-sampling weights\n",
    "        sample_weights = torch.from_numpy(sample_weights).float().unsqueeze(1).to(device)\n",
    "        loss = (sample_weights * td_errors**2).mean()\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Soft update of target model: θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        for target_param, local_param in zip(self.target_model.parameters(), self.local_model.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                self.soft_update_factor * local_param.data + (1.0 - self.soft_update_factor) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 31\tAverage Score: 0.29"
     ]
    }
   ],
   "source": [
    "def train(\n",
    "    agent,\n",
    "    max_episodes, # (int): maximum number of training episodes\n",
    "    max_steps, # (int): maximum number of steps per episode\n",
    "    goal_score, # (int): average score to achive over 100 episodes \n",
    "    epsilon_start, # (float): start value of epsilon, for epsilon-greedy action selection\n",
    "    epsilon_end, # (float): minimum value of epsilon\n",
    "    epsilon_decay, # (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    sample_exp_start, # (float): start value fo importance-sampling exponent\n",
    "    sample_exp_end,  # (float): end value fo importance-sampling exponent,\n",
    "    model_path\n",
    "):\n",
    "    scores = [] # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100) # last 100 scores\n",
    "    epsilon = epsilon_start # initialize epsilon\n",
    "    \n",
    "    sample_exp = sample_exp_start\n",
    "    sample_exp_step = (sample_exp_end - sample_exp_start) / max_episodes\n",
    "    \n",
    "    for episode in range(1, max_episodes + 1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0] # get the current state\n",
    "        score = 0 # initialize the score\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = agent.act(state, epsilon)\n",
    "            \n",
    "            env_info = env.step(action)[brain_name] # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0] # get the next state\n",
    "            reward = env_info.rewards[0] # get the reward\n",
    "            done = env_info.local_done[0] # see if episode has finished\n",
    "                \n",
    "            agent.perceive(state, action, reward, next_state, done, sample_exp)\n",
    "            \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        scores_window.append(score) # save most recent score\n",
    "        scores.append(score) # save most recent score\n",
    "        epsilon = max(epsilon_end, epsilon_decay * epsilon) # decrease epsilon\n",
    "        sample_exp += sample_exp_step # increase important-sampling exponent\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_window)), end=\"\")\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_window)))\n",
    "        \n",
    "        if np.mean(scores_window) >= goal_score:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode - 100, np.mean(scores_window)))\n",
    "            torch.save(agent.local_model.state_dict(), model_path)\n",
    "            break\n",
    "    \n",
    "    return scores\n",
    "\n",
    "agent = PriorityReplayAgent(\n",
    "    state_size=state_size, \n",
    "    action_size=action_size,\n",
    "    memory_size=int(1e5),\n",
    "    batch_size=64,\n",
    "    priority_exp=0.5,\n",
    "    sort_every=int(1e4),\n",
    "    learn_every=4,\n",
    "    learn_rate=5e-4,\n",
    "    discount_factor=0.99,\n",
    "    soft_update_factor=1e-3\n",
    ")\n",
    "\n",
    "scores = train(\n",
    "    agent=agent,  \n",
    "    max_episodes=600, \n",
    "    max_steps=1000, \n",
    "    goal_score=13, \n",
    "    epsilon_start=1.0, \n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay=0.995,\n",
    "    sample_exp_start=0,\n",
    "    sample_exp_end=1,\n",
    "    model_path='models/priority_replay_dqn.pth'\n",
    ")\n",
    "\n",
    "plot_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent, epsilon=0.01):\n",
    "    env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "    state = env_info.vector_observations[0] # get the current state\n",
    "    score = 0 # initialize the score\n",
    "\n",
    "    while True:\n",
    "        action = agent.act(state, epsilon) # select an action\n",
    "        env_info = env.step(action)[brain_name] # send the action to the environment\n",
    "        next_state = env_info.vector_observations[0] # get the next state\n",
    "        reward = env_info.rewards[0] # get the reward\n",
    "        done = env_info.local_done[0] # see if episode has finished\n",
    "        score += reward # update the score\n",
    "        state = next_state # roll over the state to next time step\n",
    "        \n",
    "        if done: # exit loop if episode finished\n",
    "            break\n",
    "    \n",
    "    print(\"Score: {}\".format(score))\n",
    "    \n",
    "test(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
